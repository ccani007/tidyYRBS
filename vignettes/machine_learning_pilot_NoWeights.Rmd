
---
title: "Risk Factors Associated with Suicide Ideation in U.S High School Students using Classification Trees"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Risk Factors Associated with Suicide Ideation in U.S High School Students using Classification Trees"}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = TRUE,
	warning = TRUE
)
```

```{r tidyverse, include=FALSE, message=FALSE}
library(conflicted)
conflict_prefer("filter", "dplyr", quiet = TRUE)
conflict_prefer("lag", "dplyr", quiet = TRUE)
suppressPackageStartupMessages(library(tidyverse))

# suppress "`summarise()` has grouped output by " messages
options(dplyr.summarise.inform=F)  
```

```{r tidymodels, include=FALSE}
suppressPackageStartupMessages(library(tidymodels))
tidymodels_prefer()
suppressMessages(conflict_prefer("spec", "yardstick"))
```

```{r setup-packages, include=FALSE}
library(tidyYRBS)
library(janitor)
library(gtsummary)
library(table1)
library(scales)
library(doParallel)
```

```{r loading-data, include=FALSE}
data("data_yrbs_2019")
```

# Background

Suicide ideation among youth are a major public health concern in the U.S.
Between 2009 and 2019, there was a significant increase in the prevalence of adolescents suidice ideation, from 13.8% to 18.8% (Jones et al., 2022). Using data from the 2019 Youth Risk Behavioral Surveillance System (YRBSS) this study aims to generate a decision tree based on CART to detect risk factors for suicide ideation. 

# Method

The data comes from the biennial YRBSS for high school students from 2019. The `tidyYRBS` package was used to extract the YRBS 2019 data and `tidyverse` was used to perform the wrangling. Finally `tidymodels` will be used to construct the classification decision tree. 

## Participants 

```{r participants}
n <- data_yrbs_2019 %>% 
  nrow() %>% 
  comma() 

```


For the 2019 YRBS the number of participants is `r n`. 

All the demographic characteristics are presented in table 1.  

It is relevant to note that when comparing the demographic data with the *Overview and Methods for the Youth Risk Behavior Surveillance System â€” United States, 2019* report there are inconsistencies regarding the percentages they report for sex and grade. For example, they report that the percentage of males is 49.4 and females is 50.6, while the report we generate the percentages are 48.6%  for males and 50.3% for females. 

However, the total number of adolescents in each group is the same for both reports. 

```{r demographics, echo=FALSE}

# This will be fixed in the wrangling of the data for the package
data_yrbs_2019 <- 
  data_yrbs_2019 %>% 
  mutate(Grade = factor(Grade))


demographics <- table1(~ Sex + Race + Grade, data = data_yrbs_2019, title = "Table 1, Demographics")
demographics

```


## CART without weights 

First, I wrangled the data. 
It is necessary to convert the variables into factors because `tidymodels` needs the variables to be defined as such. 
I got rid of all the variables related to weight because this pilot analysis will not consider them. 
Finally, I eliminated race and Q4 (asks if the adolescent is Latino) because this pilot will only consider adolescents that selected Hispanic/Latino.  

```{r wrangle}

data_yrbs_2019_race <- 
  data_yrbs_2019 %>% 
  filter(Race == "Hispanic/Latino") %>% 
  mutate(
    across(
      c(suicide_considered, suicide_planned, suicide_injury, Q19, 
        Q23, Q24, Q25, Q30, Q34, Q57, Q39, Q57, Q63, Q58, Q98, 
        Q84, Q85, Q87, Q94), 
      factor
     )
   ) %>% 
  select(- c(weight, stratum, psu, n_suicide_attempts, Race, Q4))

```

After the wrangling I splitted the data into testing (25%) and training (75%) with a stratified argument for the outcome (suicide ideation). 
The resulting data sets are presented in table 2. 

```{r split-data, include=FALSE}
set.seed(02282015)

analysis_split <-
  data_yrbs_2019_race  %>% 
  initial_split(strata = suicide_considered)

analysis_split

analysis_train <- training(analysis_split)
analysis_test <- testing(analysis_split)

# Checking the training and test data sets

n_analysis_model <- 
data_yrbs_2019 %>% 
  tabyl(suicide_considered) %>% 
  adorn_pct_formatting(0) %>% 
  adorn_totals()

n_analysis_train <- 
analysis_train %>% 
  tabyl(suicide_considered) %>% 
  adorn_pct_formatting(0) %>% 
  adorn_totals()

n_analysis_test <- 
analysis_test %>% 
  tabyl(suicide_considered) %>% 
  adorn_pct_formatting(0) %>% 
  adorn_totals()


```


```{r split-table, echo=FALSE}

table_model <- 
  data_yrbs_2019 |> 
  select(suicide_considered) 

tbl_analysis_model <- tbl_summary(table_model)

table_train <- 
  analysis_train |> 
  select(suicide_considered) 

tbl_analysis_train <- tbl_summary(table_train)

table_test <- 
  analysis_test  |> 
  select(suicide_considered)
    
tbl_analysis_test <- tbl_summary(table_test)

complete_table <- 
  tbl_merge(
    tbls = list(tbl_analysis_model, tbl_analysis_train, tbl_analysis_test), 
    tab_spanner = c("**Complete data**", "**Training data**", "**Testing data**" ) 
  ) 

complete_table

```


## Building the model with `Tidymodels`

After dividing the data, I will create the model using the three `tidymodels` steps:   
1. The recipe  
2. Setting the model   
3. Setting the workflow. 

```{r themodel, warning=FALSE}
the_recipe <- 
  recipe(formula = suicide_considered ~ ., data = analysis_train) %>%
  update_role(ID, new_role = "ID") %>%  
  step_dummy(all_nominal_predictors()) 

# x <- bake(prep(the_recipe), new_data = NULL)
# skimr::skim(x)

cart_model <- 
  decision_tree(cost_complexity = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

cart_workflow <- 
  workflow() %>% 
  add_recipe(the_recipe) %>% 
  add_model(cart_model)


```


Before running the model I will use 5-fold cross validation to find the best Complexity Parameter for the decision tree using this code:

```{r crossvalidation}

cv_folds <-
  vfold_cv(analysis_train, 
           v = 5,  # the number of sets
           strata = suicide_considered)

# use multiple CPU cores for faster processing
doParallel::registerDoParallel()  

cart_tune <- 
  cart_workflow %>% 
  tune_grid(resamples = cv_folds,
            grid = 10, 
            metrics = metric_set(roc_auc, kap),
            control = control_grid(save_pred = TRUE)
  )

# stop the use multiple CPU cores for faster processing
doParallel::stopImplicitCluster()  

```

After the crossvalidation, I proceed to evaluate and choose the best CP value by the highest value on the receiver operating characteristic curve (ROC)

``` {r choosing-bestCP, echo=FALSE}

show_best(cart_tune, metric = "roc_auc")

bestPlot_cart <- 
  autoplot(cart_tune)

bestPlot_cart

cart_best <- select_best(cart_tune, metric = "roc_auc")

```

After choosing the best CP I proceed to run the final workflow with this code:
```{r final-model, warning=FALSE}

cart_final_workflow <- 
  cart_workflow %>% 
  finalize_workflow(cart_best)

ideation_fit <- 
  cart_final_workflow |>  
  fit(data = analysis_train)

the_results <-
  cart_final_workflow %>% 
  last_fit(split = analysis_split, 
           metrics = metric_set(recall, precision, f_meas, 
                                yardstick::accuracy, kap,
                                roc_auc, sens, spec))
```
After running the model I am interested in knowing the metrics, therefore I collect them and present them in the following table 

```{r metrics, echo=FALSE} 

metrics <- 
  the_results %>%
  collect_metrics(summarize = TRUE)

metrics

```

In order to evaluate the performance of the model classifying I created a confusion matrix: 
```{r confusion-matrix}
# Prediction per person 

the_prediction <-  
  the_results %>% 
  collect_predictions()

# head(the_prediction, n = 20)

# Confusion Matrix 

confusion_matrix_cart <- 
  the_prediction %>% 
  conf_mat(suicide_considered, .pred_class)

confusion_matrixPlot_cart <- 
  autoplot(confusion_matrix_cart, type = "heatmap")

confusion_matrixPlot_cart
```

 I plotted the ROC-AUC for both the training and testing data set to evaluate the perfomance of the model. 
This is the training confusion matrix:
 
```{r roc-training}
#ROC curve training

ideation_pred_train <- 
  predict(ideation_fit, analysis_train, type = "prob") |> 
  bind_cols(analysis_train %>% select(suicide_considered)) 

rocPlot_cart_train <- 
  ideation_pred_train %>%  
  roc_curve(truth = suicide_considered, .pred_FALSE) |> 
  autoplot()

rocTable_cart_train <- 
  ideation_pred_train  %>% 
  roc_auc(truth = suicide_considered, .pred_FALSE)

rocPlot_cart_train


```

This is the testing confusion matrix 
```{r testing}
# ROC curve testing

rocPlot_cart <- 
  the_prediction %>% 
  roc_curve(suicide_considered, .pred_FALSE) %>% 
  autoplot() 

rocTable_cart <- 
  the_prediction %>%  
  roc_auc(truth = suicide_considered, .pred_FALSE)

rocPlot_cart
```

Finally I plotted the decision tree

```{r the-tree, echo=FALSE}

cart_trained <- 
  the_results %>% 
  extract_fit_parsnip()

cart_tree_fit <- cart_trained$fit

rpart.plot::rpart.plot(cart_tree_fit, roundint = FALSE)

# treemisc::tree_diagram(cart_tree_fit, roundint=FALSE)

```

