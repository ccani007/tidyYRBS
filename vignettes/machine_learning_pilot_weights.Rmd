
---
title: "Risk Factors Associated with Suicide Attempts in U.S High School Students using Machine Learning Tools"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Risk Factors Associated with Suicide Attempts in U.S High School Students using Machine Learning Tools}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = TRUE,
	warning = TRUE
)
```

```{r tidyverse, include=FALSE, message=FALSE}
library(conflicted)
conflict_prefer("filter", "dplyr", quiet = TRUE)
conflict_prefer("lag", "dplyr", quiet = TRUE)
suppressPackageStartupMessages(library(tidyverse))

# suppress "`summarise()` has grouped output by " messages
options(dplyr.summarise.inform=F)  
```

```{r tidymodels, include=FALSE}
suppressPackageStartupMessages(library(tidymodels))
tidymodels_prefer()
suppressMessages(conflict_prefer("spec", "yardstick"))
```

```{r setup-packages, include=FALSE}
library(tidyYRBS)
library(srvyr)
library(survey)
library(janitor)
library(gtsummary)
library(parsnip)
library(hardhat)
```

```{r loading-data, include=FALSE}
data("clean_yrbs_2019")
```

# Background

Suicide ideation among youth are a major public health concern in the U.S.. Between 2009 and 2019, there was a significant increase in the prevalence of adolescents suidice ideation (Jones et al., 2022). Using data from the 2019 Youth Risk Behavioral Surveillance System (YRBSS) this study aims to generate a decision tree based on CART to detect risk factors for suicide attempt. 

# Method
The data comes from the biennial YRBSS for high school students from 2019. The `tidyYRBS` package was used to analyze the data in R studio and perform the wrangling. 

```{r demographics, include=FALSE}
n <- clean_yrbs_2019 %>% nrow()

```

The number of adolescents included in this study is N = `r format(c(n), big.mark = ",")`. 


```{r weights, include=FALSE}

# This function transforms the Data Frame into a survey object
yrbs_2019_srv <-
  clean_yrbs_2019 %>%
  srvyr::as_survey_design(
    ids     = psu,
    weights = weight,
    strata  = stratum,
    nest    = TRUE
  )

# Cheking the amount of observations

total_weight <- 
  yrbs_2019_srv  %>% 
  summarise(N = survey_total())


# Checking the data to compare with the official results. 
# There are differences, but it looks like the result is not taking into account the 
# weights, at least for sex

# https://www.cdc.gov/healthyyouth/data/yrbs/pdf/2019/su6901-H.pdf

by_sex <- 
  yrbs_2019_srv %>% 
  group_by(Sex) %>%
  summarise(N = survey_total())

# Total people: 6690.2341(F) + 6861.9249(M) + 124.8481 (NA) = 13677 total
# By sex there are differences in the amount 

suicide_considered <- 
  yrbs_2019_srv %>% 
  group_by(suicide_considered) %>%
  summarise(proportion = survey_mean(),
            total = survey_total())
# True: 2527.3820 (18.4%) | False: 10950.3745 (80.06%) | NA: 199 (1.4%) 

suicide_attempt <- 
  yrbs_2019_srv %>% 
  group_by(suicide_attempts) %>%
  summarise(proportion = survey_mean(),
            total = survey_total())
# True: 1017.866 (7.4%) | False: 10411.594 (76.12%) | NA: 2247 (16%) 

suicide_planned <- 
  yrbs_2019_srv %>% 
  group_by(suicide_planned) %>%
  summarise(proportion = survey_mean(),
            total = survey_total())
# True: 2117.3515 (15.5%) | False: 11360.0414 (83.05%) | NA: 199.6142 (1%) 

suicide_injury <- 
  yrbs_2019_srv %>% 
  group_by(suicide_injury) %>%
  summarise(proportion = survey_mean(),
            total = survey_total())
# True: 268.2899 (1.9%) | False: 10359.5872	 (75.57%) | NA: 3049.1300 (2.2%) 
```


## CART with weights 

First, I wrangled the data. 
It is necessary to convert the variables into factors because `tidymodels` needs the variables to be defined as such. 
I created the weights as importance weights with the `hardhat` package to be taken into account in the reciepe. 
Finally, I eliminated race and Q4 (asks if the adolescent is Latino) because this pilot will only consider adolescents that selected Hispanic/Latino.  

```{r wrangle}
data_yrbs_2019_race <- 
  clean_yrbs_2019 %>% 
  filter(Race == "Hispanic/Latino") %>% 
  mutate(
    across(
      c(suicide_considered, suicide_planned, suicide_injury, Q19, 
        Q23, Q24, Q25, Q30, Q34, Q57, Q39, Q57, Q63, Q58, Q98, 
        Q84, Q85, Q87), 
      factor
     )
   ) %>% 
  filter(!is.na(suicide_considered)) %>% 
  mutate(case_wts = importance_weights(weight)) %>% 
  select(- c(stratum, psu, n_suicide_attempts, Race, Q4,suicide_planned, 
             suicide_attempts, suicide_injury, weight))

```

After the wrangling I splitted the data into training (75%) and testing (25%) with a stratified argument for the outcome (suicide ideation). 

```{r split-data}
set.seed(1234)

analysis_split <-
  data_yrbs_2019_race %>% 
  initial_split(strata = suicide_considered)

analysis_train <- training(analysis_split)
analysis_test <- testing(analysis_split)

# Checking the training and test data sets

n_analysis_model <- 
data_yrbs_2019_race  %>% 
  tabyl(suicide_considered) %>% 
  adorn_pct_formatting(0) %>% 
  adorn_totals()

n_analysis_model

n_analysis_train <- 
analysis_train %>% 
  tabyl(suicide_considered) %>% 
  adorn_pct_formatting(0) %>% 
  adorn_totals()

n_analysis_train

n_analysis_test <- 
analysis_test %>% 
  tabyl(suicide_considered) %>% 
  adorn_pct_formatting(0) %>% 
  adorn_totals()

n_analysis_test

```
After splitting I created the crossvalidation set only for the training data. 
```{r}
set.seed(2)
yrbs_folds <- vfold_cv(analysis_train , strata = suicide_considered)
```



## Building the CART model with `Tidymodels`

After dividing the data, I will create the model using the three `tidymodels` steps:   
1. Setting the model   
2. The recipe
3. Setting the workflow. 


```{r themodel, warning=FALSE}
# Tidy model steps 
YRBS_spec <- 
  decision_tree(cost_complexity = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

YRBS_rec <- 
  recipe(suicide_considered ~ ., data = analysis_train) %>% 
  step_normalize(all_numeric_predictors())

cart_workflow <- 
  workflow() %>% 
  add_recipe(YRBS_rec) %>% 
  add_model(YRBS_spec) %>% 
  add_case_weights(case_wts)




```


Before running the model I will use 10-fold cross validation created before to find the best Complexity Parameter for the decision tree using this code:

```{r crossvalidation}

# use multiple CPU cores for faster processing
doParallel::registerDoParallel()  

cart_tune <- 
  cart_workflow %>% 
  tune_grid(resamples = yrbs_folds,
            grid = 10, 
            metrics = metric_set(roc_auc, kap),
            control = control_grid(save_pred = TRUE)
  )

# stop the use multiple CPU cores for faster processing
doParallel::stopImplicitCluster()  

```

After the crossvalidation, I proceed to evaluate and choose the best CP value by the highest value on the receiver operating characteristic curve (ROC)

``` {r choosing-bestCP, dev='svg'}

show_best(cart_tune, metric = "roc_auc")

bestPlot_cart <- autoplot(cart_tune)
bestPlot_cart

cart_best <- select_best(cart_tune, metric = "roc_auc")
cart_best
```

After choosing the best CP I proceed to run the final workflow with this code:
```{r final-model, warning=FALSE}

cart_final_workflow <- 
  cart_workflow %>% 
  finalize_workflow(cart_best)

# Ideation fit is the object that will fit the final workflow in the 
#  Training data set. 
ideation_fit <- 
  cart_final_workflow %>%   
  fit(data = analysis_train)


the_results <-
  cart_final_workflow %>% 
  last_fit(split = analysis_split, 
           metrics = metric_set(recall, precision, f_meas, 
                                yardstick::accuracy, kap,
                                roc_auc, sens, spec))
```
After running the model I am interested in knowing the metrics, therefore I collect them and present them in the following table 

```{r metrics, echo=FALSE} 

metrics <- 
  the_results %>%
  collect_metrics(summarize = TRUE)

metrics

```

In order to evaluate the performance of the model classifying I created a confusion matrix: 
```{r confusion-matrix, dev='svg', warning=FALSE}
# Prediction per person 

the_prediction <-  
  the_results %>% 
  collect_predictions()

# head(the_prediction, n = 20)

# Confusion Matrix 

confusion_matrix_cart <- 
  the_prediction %>% 
  conf_mat(suicide_considered, .pred_class)

confusion_matrixPlot_cart <- 
  autoplot(confusion_matrix_cart, type = "heatmap")

confusion_matrixPlot_cart
```

 I plotted the ROC-AUC for both the training and testing data set to evaluate the performance of the model. 
This is the training confusion matrix:
 
```{r roc-training, dev='svg', warning=FALSE}
#ROC curve training

ideation_pred_train <- 
  predict(ideation_fit, analysis_train, type = "prob") |> 
  bind_cols(analysis_train %>% select(suicide_considered)) 

rocPlot_cart_train <- 
  ideation_pred_train %>%  
  roc_curve(truth = suicide_considered, .pred_FALSE) |> 
  autoplot()

rocTable_cart_train <- 
  ideation_pred_train  %>% 
  roc_auc(truth = suicide_considered, .pred_FALSE)

rocPlot_cart_train
rocTable_cart_train


```

This is the testing confusion matrix 
```{r testing, dev='svg'}
# ROC curve testing

rocPlot_cart <- 
  the_prediction %>% 
  roc_curve(suicide_considered, .pred_FALSE) %>% 
  autoplot() 

rocTable_cart <- 
  the_prediction %>%  
  roc_auc(truth = suicide_considered, .pred_FALSE)

rocPlot_cart
rocTable_cart
```

Finally I plotted the decision tree

```{r the-tree, dev='svg'}

cart_trained <- 
  the_results %>% 
  extract_fit_parsnip()

cart_tree_fit <- cart_trained$fit

treemisc::tree_diagram(cart_tree_fit, roundint=FALSE)

```
